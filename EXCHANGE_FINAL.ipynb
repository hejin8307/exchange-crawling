{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['미국', '2019.11.21', '1,175.50', '1,196.07', '1,154.93', '1,187.00', '1,164.00', '1,189.60', '1,162.84']\n",
      "['미국', '2019.11.20', '1,170.50', '1,190.98', '1,150.02', '1,181.90', '1,159.10', '1,184.54', '1,157.94']\n",
      "['미국', '2019.11.19', '1,166.50', '1,186.91', '1,146.09', '1,177.90', '1,155.10', '1,180.49', '1,153.95']\n",
      "['미국', '2019.11.18', '1,165.50', '1,185.89', '1,145.11', '1,176.90', '1,154.10', '1,179.48', '1,152.95']\n",
      "['미국', '2019.11.15', '1,167.00', '1,187.42', '1,146.58', '1,178.40', '1,155.60', '1,181.00', '1,154.43']\n",
      "['미국', '2019.11.14', '1,170.50', '1,190.98', '1,150.02', '1,181.90', '1,159.10', '1,184.54', '1,157.93']\n",
      "['미국', '2019.11.13', '1,169.50', '1,189.96', '1,149.04', '1,180.90', '1,158.10', '1,183.53', '1,156.93']\n",
      "['미국', '2019.11.12', '1,163.50', '1,183.86', '1,143.14', '1,174.90', '1,152.10', '1,177.46', '1,150.94']\n",
      "['미국', '2019.11.11', '1,165.50', '1,185.89', '1,145.11', '1,176.90', '1,154.10', '1,179.48', '1,152.94']\n",
      "['미국', '2019.11.08', '1,157.50', '1,177.75', '1,137.25', '1,168.80', '1,146.20', '1,171.39', '1,145.05']\n",
      "['일본', '2019.11.21', '1,082.26', '1,101.19', '1,063.33', '1,092.86', '1,071.66', '1,093.08', '1,071.16']\n",
      "['일본', '2019.11.20', '1,079.35', '1,098.23', '1,060.47', '1,089.92', '1,068.78', '1,090.14', '1,068.28']\n",
      "['일본', '2019.11.19', '1,072.05', '1,090.81', '1,053.29', '1,082.55', '1,061.55', '1,082.77', '1,061.05']\n",
      "['일본', '2019.11.18', '1,068.82', '1,087.52', '1,050.12', '1,079.29', '1,058.35', '1,079.50', '1,057.86']\n",
      "['일본', '2019.11.15', '1,074.14', '1,092.93', '1,055.35', '1,084.66', '1,063.62', '1,084.88', '1,063.12']\n",
      "['일본', '2019.11.14', '1,077.36', '1,096.21', '1,058.51', '1,087.91', '1,066.81', '1,088.13', '1,066.31']\n",
      "['일본', '2019.11.13', '1,073.67', '1,092.45', '1,054.89', '1,084.19', '1,063.15', '1,084.40', '1,062.65']\n",
      "['일본', '2019.11.12', '1,065.72', '1,084.37', '1,047.07', '1,076.16', '1,055.28', '1,076.37', '1,054.79']\n",
      "['일본', '2019.11.11', '1,069.51', '1,088.22', '1,050.80', '1,079.99', '1,059.03', '1,080.20', '1,058.53']\n",
      "['일본', '2019.11.08', '1,058.48', '1,077.00', '1,039.96', '1,068.85', '1,048.11', '1,069.06', '1,047.62']\n",
      "['유럽연합', '2019.11.21', '1,302.63', '1,328.55', '1,276.71', '1,315.65', '1,289.61', '1,322.16', '1,289.08']\n",
      "['유럽연합', '2019.11.20', '1,295.22', '1,320.99', '1,269.45', '1,308.17', '1,282.27', '1,314.64', '1,281.74']\n",
      "['유럽연합', '2019.11.19', '1,291.37', '1,317.06', '1,265.68', '1,304.28', '1,278.46', '1,310.74', '1,277.94']\n",
      "['유럽연합', '2019.11.18', '1,288.87', '1,314.51', '1,263.23', '1,301.75', '1,275.99', '1,308.20', '1,275.47']\n",
      "['유럽연합', '2019.11.15', '1,287.20', '1,312.81', '1,261.59', '1,300.07', '1,274.33', '1,306.50', '1,273.81']\n",
      "['유럽연합', '2019.11.14', '1,287.37', '1,312.98', '1,261.76', '1,300.24', '1,274.50', '1,306.68', '1,273.98']\n",
      "['유럽연합', '2019.11.13', '1,288.09', '1,313.72', '1,262.46', '1,300.97', '1,275.21', '1,307.41', '1,274.69']\n",
      "['유럽연합', '2019.11.12', '1,282.53', '1,308.05', '1,257.01', '1,295.35', '1,269.71', '1,301.76', '1,269.19']\n",
      "['유럽연합', '2019.11.11', '1,285.90', '1,311.48', '1,260.32', '1,298.75', '1,273.05', '1,305.18', '1,272.53']\n",
      "['유럽연합', '2019.11.08', '1,277.24', '1,302.65', '1,251.83', '1,290.01', '1,264.47', '1,296.39', '1,263.95']\n",
      "['중국', '2019.11.21', '167.17', '175.52', '158.82', '168.84', '165.50', 'N/A', 'N/A']\n",
      "['중국', '2019.11.20', '166.34', '174.65', '158.03', '168.00', '164.68', 'N/A', 'N/A']\n",
      "['중국', '2019.11.19', '166.04', '174.34', '157.74', '167.70', '164.38', 'N/A', 'N/A']\n",
      "['중국', '2019.11.18', '166.11', '174.41', '157.81', '167.77', '164.45', 'N/A', 'N/A']\n",
      "['중국', '2019.11.15', '166.42', '174.74', '158.10', '168.08', '164.76', 'N/A', 'N/A']\n",
      "['중국', '2019.11.14', '166.67', '175.00', '158.34', '168.33', '165.01', 'N/A', 'N/A']\n",
      "['중국', '2019.11.13', '166.49', '174.81', '158.17', '168.15', '164.83', 'N/A', 'N/A']\n",
      "['중국', '2019.11.12', '166.01', '174.31', '157.71', '167.67', '164.35', 'N/A', 'N/A']\n",
      "['중국', '2019.11.11', '166.26', '174.57', '157.95', '167.92', '164.60', 'N/A', 'N/A']\n",
      "['중국', '2019.11.08', '165.66', '173.94', '157.38', '167.31', '164.01', 'N/A', 'N/A']\n",
      "['캐나다', '2019.11.21', '883.64', '901.04', '866.24', '892.47', '874.81', '896.89', '873.87']\n",
      "['캐나다', '2019.11.20', '879.28', '896.60', '861.96', '888.07', '870.49', '892.46', '869.55']\n",
      "['캐나다', '2019.11.19', '883.28', '900.68', '865.88', '892.11', '874.45', '896.52', '873.51']\n",
      "['캐나다', '2019.11.18', '881.65', '899.01', '864.29', '890.46', '872.84', '894.87', '871.90']\n",
      "['캐나다', '2019.11.15', '880.92', '898.27', '863.57', '889.72', '872.12', '894.13', '871.18']\n",
      "['캐나다', '2019.11.14', '882.40', '899.78', '865.02', '891.22', '873.58', '895.63', '872.64']\n",
      "['캐나다', '2019.11.13', '881.74', '899.11', '864.37', '890.55', '872.93', '894.96', '871.99']\n",
      "['캐나다', '2019.11.12', '878.01', '895.30', '860.72', '886.79', '869.23', '891.18', '868.29']\n",
      "['캐나다', '2019.11.11', '881.49', '898.85', '864.13', '890.30', '872.68', '894.71', '871.74']\n",
      "['캐나다', '2019.11.08', '876.86', '894.13', '859.59', '885.62', '868.10', '890.01', '867.16']\n",
      "['호주', '2019.11.21', '800.46', '816.22', '784.70', '808.46', '792.46', '812.46', '791.83']\n",
      "['호주', '2019.11.20', '797.29', '812.99', '781.59', '805.26', '789.32', '809.24', '788.70']\n",
      "['호주', '2019.11.19', '795.49', '811.16', '779.82', '803.44', '787.54', '807.42', '786.92']\n",
      "['호주', '2019.11.18', '793.65', '809.28', '778.02', '801.58', '785.72', '805.55', '785.09']\n",
      "['호주', '2019.11.15', '792.45', '808.06', '776.84', '800.37', '784.53', '804.33', '783.89']\n",
      "['호주', '2019.11.14', '795.00', '810.66', '779.34', '802.95', '787.05', '806.92', '786.42']\n",
      "['호주', '2019.11.13', '798.42', '814.14', '782.70', '806.40', '790.44', '810.39', '789.80']\n",
      "['호주', '2019.11.12', '795.19', '810.85', '779.53', '803.14', '787.24', '807.11', '786.61']\n",
      "['호주', '2019.11.11', '799.94', '815.69', '784.19', '807.93', '791.95', '811.93', '791.31']\n",
      "['호주', '2019.11.08', '794.68', '810.33', '779.03', '802.62', '786.74', '806.60', '786.10']\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "import requests\n",
    "import pandas as pd\n",
    "import csv\n",
    "import datetime\n",
    "\n",
    "URL = 'http://finance.naver.com/marketindex/exchangeDailyQuote.nhn?marketindexCd=FX_'\n",
    "keys = ['USDKRW', 'JPYKRW', 'EURKRW', 'CNYKRW', 'CADKRW', 'AUDKRW']\n",
    "\n",
    "def country():\n",
    "    country_name_list = []\n",
    "    for key in keys:\n",
    "        country_url = 'https://finance.naver.com/marketindex/exchangeDetail.nhn?marketindexCd=FX_'\n",
    "        tempUrl = country_url + key\n",
    "        req = requests.get(tempUrl)\n",
    "        soup = BeautifulSoup(req.text, \"html.parser\")\n",
    "        country_names = soup.select('#container > div.h_company > h2 ')\n",
    "        for country_name in country_names:\n",
    "#             print(country_name.text)\n",
    "            country_name = country_name.text.strip()\n",
    "            country_name_list.append(country_name)\n",
    "    return country_name_list\n",
    "\n",
    "def usd(soup, country_name):\n",
    "#     url = URL + keys[0]\n",
    "#     for i in range(1, 5):\n",
    "#         usd_url = url + '&page=' + str(i)\n",
    "#         req = requests.get(usd_url)\n",
    "#         soup = BeautifulSoup(req.text, \"html.parser\")\n",
    "#         sleep(1)\n",
    "    country()\n",
    "    usd_list = []\n",
    "    tr_list = soup.select('body > div > table > tbody > tr')\n",
    "    for tr in tr_list:\n",
    "        usd_date = tr.find('td', {'class' : 'date'}).text\n",
    "        usd_exchange = tr.find('td', {'class' : 'num'}).text\n",
    "        for usd_buy_cash in tr.select('td:nth-child(4)'):\n",
    "            usd_buy_cash = usd_buy_cash.text\n",
    "        for usd_sell_cash in tr.select('td:nth-child(5)'):\n",
    "            usd_sell_cash = usd_sell_cash.text\n",
    "        for usd_send_money in tr.select('td:nth-child(6)'):\n",
    "            usd_send_money = usd_send_money.text\n",
    "        for usd_get_money in tr.select('td:nth-child(7)'):\n",
    "            usd_get_money = usd_get_money.text\n",
    "        for usd_buy_tc in tr.select('td:nth-child(8)'):\n",
    "            usd_buy_tc = usd_buy_tc.text.strip()\n",
    "        for usd_sell_check in tr.select('td:nth-child(9)'):\n",
    "            usd_sell_check = usd_sell_check.text.strip()\n",
    "        usd_list.append([country_name, usd_date, usd_exchange, usd_buy_cash, usd_sell_cash, usd_send_money, usd_get_money, usd_buy_tc, usd_sell_check])\n",
    "\n",
    "    return usd_list\n",
    "\n",
    "def jpy(soup, country_name):\n",
    "#     url = URL + keys[0]\n",
    "#     for i in range(1, 5):\n",
    "#         usd_url = url + '&page=' + str(i)\n",
    "#         req = requests.get(usd_url)\n",
    "#         soup = BeautifulSoup(req.text, \"html.parser\")\n",
    "#         sleep(1)\n",
    "    country()\n",
    "    jpy_list = []\n",
    "    tr_list = soup.select('body > div > table > tbody > tr')\n",
    "    for tr in tr_list:\n",
    "        jpy_date = tr.find('td', {'class' : 'date'}).text\n",
    "        jpy_exchange = tr.find('td', {'class' : 'num'}).text\n",
    "        for jpy_buy_cash in tr.select('td:nth-child(4)'):\n",
    "            jpy_buy_cash = jpy_buy_cash.text\n",
    "        for jpy_sell_cash in tr.select('td:nth-child(5)'):\n",
    "            jpy_sell_cash = jpy_sell_cash.text\n",
    "        for jpy_send_money in tr.select('td:nth-child(6)'):\n",
    "            jpy_send_money = jpy_send_money.text\n",
    "        for jpy_get_money in tr.select('td:nth-child(7)'):\n",
    "            jpy_get_money = jpy_get_money.text\n",
    "        for jpy_buy_tc in tr.select('td:nth-child(8)'):\n",
    "            jpy_buy_tc = jpy_buy_tc.text.strip()\n",
    "        for jpy_sell_check in tr.select('td:nth-child(9)'):\n",
    "            jpy_sell_check = jpy_sell_check.text.strip()\n",
    "        jpy_list.append([country_name, jpy_date, jpy_exchange, jpy_buy_cash, jpy_sell_cash, jpy_send_money, jpy_get_money, jpy_buy_tc, jpy_sell_check])\n",
    "\n",
    "    return jpy_list\n",
    "\n",
    "def eur(soup, country_name):\n",
    "#     url = URL + keys[0]\n",
    "#     for i in range(1, 5):\n",
    "#         usd_url = url + '&page=' + str(i)\n",
    "#         req = requests.get(usd_url)\n",
    "#         soup = BeautifulSoup(req.text, \"html.parser\")\n",
    "#         sleep(1)\n",
    "    country()\n",
    "    eur_list = []\n",
    "    tr_list = soup.select('body > div > table > tbody > tr')\n",
    "    for tr in tr_list:\n",
    "        eur_date = tr.find('td', {'class' : 'date'}).text\n",
    "        eur_exchange = tr.find('td', {'class' : 'num'}).text\n",
    "        for eur_buy_cash in tr.select('td:nth-child(4)'):\n",
    "            eur_buy_cash = eur_buy_cash.text\n",
    "        for eur_sell_cash in tr.select('td:nth-child(5)'):\n",
    "            eur_sell_cash = eur_sell_cash.text\n",
    "        for eur_send_money in tr.select('td:nth-child(6)'):\n",
    "            eur_send_money = eur_send_money.text\n",
    "        for eur_get_money in tr.select('td:nth-child(7)'):\n",
    "            eur_get_money = eur_get_money.text\n",
    "        for eur_buy_tc in tr.select('td:nth-child(8)'):\n",
    "            eur_buy_tc = eur_buy_tc.text.strip()\n",
    "        for eur_sell_check in tr.select('td:nth-child(9)'):\n",
    "            eur_sell_check = eur_sell_check.text.strip()\n",
    "        eur_list.append([country_name, eur_date, eur_exchange, eur_buy_cash, eur_sell_cash, eur_send_money, eur_get_money, eur_buy_tc, eur_sell_check])\n",
    "\n",
    "    return eur_list\n",
    "\n",
    "def cny(soup, country_name):\n",
    "#     url = URL + keys[0]\n",
    "#     for i in range(1, 5):\n",
    "#         usd_url = url + '&page=' + str(i)\n",
    "#         req = requests.get(usd_url)\n",
    "#         soup = BeautifulSoup(req.text, \"html.parser\")\n",
    "#         sleep(1)\n",
    "    country()\n",
    "    cny_list = []\n",
    "    tr_list = soup.select('body > div > table > tbody > tr')\n",
    "    for tr in tr_list:\n",
    "        cny_date = tr.find('td', {'class' : 'date'}).text\n",
    "        cny_exchange = tr.find('td', {'class' : 'num'}).text\n",
    "        for cny_buy_cash in tr.select('td:nth-child(4)'):\n",
    "            cny_buy_cash = cny_buy_cash.text\n",
    "        for cny_sell_cash in tr.select('td:nth-child(5)'):\n",
    "            cny_sell_cash = cny_sell_cash.text\n",
    "        for cny_send_money in tr.select('td:nth-child(6)'):\n",
    "            cny_send_money = cny_send_money.text\n",
    "        for cny_get_money in tr.select('td:nth-child(7)'):\n",
    "            cny_get_money = cny_get_money.text\n",
    "        for cny_buy_tc in tr.select('td:nth-child(8)'):\n",
    "            cny_buy_tc = cny_buy_tc.text.strip()\n",
    "        for cny_sell_check in tr.select('td:nth-child(9)'):\n",
    "            cny_sell_check = cny_sell_check.text.strip()\n",
    "        cny_list.append([country_name, cny_date, cny_exchange, cny_buy_cash, cny_sell_cash, cny_send_money, cny_get_money, cny_buy_tc, cny_sell_check])\n",
    "\n",
    "    return cny_list\n",
    "\n",
    "def cad(soup, country_name):\n",
    "#     url = URL + keys[0]\n",
    "#     for i in range(1, 5):\n",
    "#         usd_url = url + '&page=' + str(i)\n",
    "#         req = requests.get(usd_url)\n",
    "#         soup = BeautifulSoup(req.text, \"html.parser\")\n",
    "#         sleep(1)\n",
    "    country()\n",
    "    cad_list = []\n",
    "    tr_list = soup.select('body > div > table > tbody > tr')\n",
    "    for tr in tr_list:\n",
    "        cad_date = tr.find('td', {'class' : 'date'}).text\n",
    "        cad_exchange = tr.find('td', {'class' : 'num'}).text\n",
    "        for cad_buy_cash in tr.select('td:nth-child(4)'):\n",
    "            cad_buy_cash = cad_buy_cash.text\n",
    "        for cad_sell_cash in tr.select('td:nth-child(5)'):\n",
    "            cad_sell_cash = cad_sell_cash.text\n",
    "        for cad_send_money in tr.select('td:nth-child(6)'):\n",
    "            cad_send_money = cad_send_money.text\n",
    "        for cad_get_money in tr.select('td:nth-child(7)'):\n",
    "            cad_get_money = cad_get_money.text\n",
    "        for cad_buy_tc in tr.select('td:nth-child(8)'):\n",
    "            cad_buy_tc = cad_buy_tc.text.strip()\n",
    "        for cad_sell_check in tr.select('td:nth-child(9)'):\n",
    "            cad_sell_check = cad_sell_check.text.strip()\n",
    "        cad_list.append([country_name, cad_date, cad_exchange, cad_buy_cash, cad_sell_cash, cad_send_money, cad_get_money, cad_buy_tc, cad_sell_check])\n",
    "\n",
    "    return cad_list\n",
    "\n",
    "def aud(soup, country_name):\n",
    "#     url = URL + keys[0]\n",
    "#     for i in range(1, 5):\n",
    "#         usd_url = url + '&page=' + str(i)\n",
    "#         req = requests.get(usd_url)\n",
    "#         soup = BeautifulSoup(req.text, \"html.parser\")\n",
    "#         sleep(1)\n",
    "    country()\n",
    "    aud_list = []\n",
    "    tr_list = soup.select('body > div > table > tbody > tr')\n",
    "    for tr in tr_list:\n",
    "        aud_date = tr.find('td', {'class' : 'date'}).text\n",
    "        aud_exchange = tr.find('td', {'class' : 'num'}).text\n",
    "        for aud_buy_cash in tr.select('td:nth-child(4)'):\n",
    "            aud_buy_cash = aud_buy_cash.text\n",
    "        for aud_sell_cash in tr.select('td:nth-child(5)'):\n",
    "            aud_sell_cash = aud_sell_cash.text\n",
    "        for aud_send_money in tr.select('td:nth-child(6)'):\n",
    "            aud_send_money = aud_send_money.text\n",
    "        for aud_get_money in tr.select('td:nth-child(7)'):\n",
    "            aud_get_money = aud_get_money.text\n",
    "        for aud_buy_tc in tr.select('td:nth-child(8)'):\n",
    "            aud_buy_tc = aud_buy_tc.text.strip()\n",
    "        for aud_sell_check in tr.select('td:nth-child(9)'):\n",
    "            aud_sell_check = aud_sell_check.text.strip()\n",
    "        aud_list.append([country_name, aud_date, aud_exchange, aud_buy_cash, aud_sell_cash, aud_send_money, aud_get_money, aud_buy_tc, aud_sell_check])\n",
    "\n",
    "    return aud_list\n",
    "\n",
    "def toCSV(whole_list):\n",
    "    dt = datetime.datetime.now()\n",
    "    file = open('test '+dt.strftime('%Y_%m_%d')+'.csv', 'w', encoding='cp949', newline='') #usd.csv로 바꿀것\n",
    "    csvfile = csv.writer(file)\n",
    "    csvfile.writerow(['country', 'date', 'exchange', 'buying cash', 'selling cash', 'sending money', 'getting money', 'buying TC', 'selling check'])\n",
    "    for row in whole_list:\n",
    "        csvfile.writerow(row)\n",
    "    file.close()\n",
    "    \n",
    "def toCSV_a(whole_list):\n",
    "    dt = datetime.datetime.now()\n",
    "    file = open('test '+dt.strftime('%Y_%m_%d')+'.csv', 'a', encoding='cp949', newline='') #usd.csv로 바꿀것\n",
    "    csvfile = csv.writer(file)\n",
    "#     csvfile.writerow(['country', 'date', 'exchange', 'buying cash', 'selling cash', 'sending money', 'getting money', 'buying TC', 'selling check'])\n",
    "    for row in whole_list:\n",
    "        csvfile.writerow(row)\n",
    "    file.close()\n",
    "\n",
    "def toCSV_new_a(whole_list):\n",
    "    dt = datetime.datetime.now()\n",
    "    with open('test '+dt.strftime('%Y_%m_%d')+'.csv', 'a', encoding='cp949', newline='') as file: #usd.csv로 바꿀것\n",
    "        csvfile = csv.writer(file)\n",
    "#         csvfile.writerow(['country', 'date', 'exchange', 'buying cash', 'selling cash', 'sending money', 'getting money', 'buying TC', 'selling check'])\n",
    "        csvfile.writerow(whole_list)\n",
    "    file.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "#     req = requests.get(URL + keys[0]) #0으로 바꿀것\n",
    "#     soup = BeautifulSoup(req.text, \"html.parser\")\n",
    "\n",
    "    country_name = country()\n",
    "\n",
    "    usd_whole_list = []\n",
    "    jpy_whole_list = []\n",
    "    eur_whole_list = []\n",
    "    cny_whole_list = []\n",
    "    cad_whole_list = []\n",
    "    aud_whole_list = []\n",
    "    \n",
    "    for i in range(1,2):\n",
    "        usd_url = URL + keys[0] + '&page=' + str(i)\n",
    "        req = requests.get(usd_url)\n",
    "        soup = BeautifulSoup(req.text, \"html.parser\")\n",
    "        sleep(1)\n",
    "        usd_whole_list += usd(soup, country_name[0])\n",
    "#         usd_whole_list += usd(soup, keys[0]) #USDKRW로 출력\n",
    "\n",
    "    \n",
    "    for i in range(1,2):\n",
    "        jpy_url = URL + keys[1] + '&page=' + str(i)\n",
    "        req = requests.get(jpy_url)\n",
    "        soup = BeautifulSoup(req.text, \"html.parser\")\n",
    "        sleep(1)\n",
    "        jpy_whole_list += jpy(soup, country_name[1])\n",
    "        \n",
    "    \n",
    "    for i in range(1,2):\n",
    "        eur_url = URL + keys[2] + '&page=' + str(i)\n",
    "        req = requests.get(eur_url)\n",
    "        soup = BeautifulSoup(req.text, \"html.parser\")\n",
    "        sleep(1)\n",
    "        eur_whole_list += eur(soup, country_name[2])\n",
    "        \n",
    "    \n",
    "    for i in range(1,2):\n",
    "        cny_url = URL + keys[3] + '&page=' + str(i)\n",
    "        req = requests.get(cny_url)\n",
    "        soup = BeautifulSoup(req.text, \"html.parser\")\n",
    "        sleep(1)\n",
    "        cny_whole_list += cny(soup, country_name[3])\n",
    "        \n",
    "    \n",
    "    for i in range(1,2):\n",
    "        usd_url = URL + keys[4] + '&page=' + str(i)\n",
    "        req = requests.get(usd_url)\n",
    "        soup = BeautifulSoup(req.text, \"html.parser\")\n",
    "        sleep(1)\n",
    "        cad_whole_list += cad(soup, country_name[4])\n",
    "        \n",
    "    \n",
    "    for i in range(1,2):\n",
    "        aud_url = URL + keys[5] + '&page=' + str(i)\n",
    "        req = requests.get(aud_url)\n",
    "        soup = BeautifulSoup(req.text, \"html.parser\")\n",
    "        sleep(1)\n",
    "        aud_whole_list += aud(soup, country_name[5])\n",
    "    \n",
    "\n",
    "    for item in usd_whole_list:\n",
    "        print(item)\n",
    "        \n",
    "    for item in jpy_whole_list:\n",
    "        print(item)\n",
    "        \n",
    "    for item in eur_whole_list:\n",
    "        print(item)\n",
    "        \n",
    "    for item in cny_whole_list:\n",
    "        print(item)\n",
    "        \n",
    "    for item in cad_whole_list:\n",
    "        print(item)\n",
    "        \n",
    "    for item in aud_whole_list:\n",
    "        print(item)\n",
    "\n",
    "    toCSV(usd_whole_list)\n",
    "    toCSV_a(jpy_whole_list)\n",
    "    toCSV_a(eur_whole_list)\n",
    "    toCSV_a(cny_whole_list)\n",
    "    toCSV_a(cad_whole_list)\n",
    "    toCSV_a(aud_whole_list)\n",
    "\n",
    "#for문 범위 수정하고 csv 파일명 수정\n",
    "#crawling_beautifulsoup에서 usd_get_one_data 함수 가져올 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
