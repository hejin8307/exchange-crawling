{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['호주', '2019.11.21', '799.68', '815.43', '783.93', '807.67', '791.69', '811.67', '791.07']\n",
      "['호주', '2019.11.20', '797.29', '812.99', '781.59', '805.26', '789.32', '809.24', '788.70']\n",
      "['호주', '2019.11.19', '795.49', '811.16', '779.82', '803.44', '787.54', '807.42', '786.92']\n",
      "['호주', '2019.11.18', '793.65', '809.28', '778.02', '801.58', '785.72', '805.55', '785.09']\n",
      "['호주', '2019.11.15', '792.45', '808.06', '776.84', '800.37', '784.53', '804.33', '783.89']\n",
      "['호주', '2019.11.14', '795.00', '810.66', '779.34', '802.95', '787.05', '806.92', '786.42']\n",
      "['호주', '2019.11.13', '798.42', '814.14', '782.70', '806.40', '790.44', '810.39', '789.80']\n",
      "['호주', '2019.11.12', '795.19', '810.85', '779.53', '803.14', '787.24', '807.11', '786.61']\n",
      "['호주', '2019.11.11', '799.94', '815.69', '784.19', '807.93', '791.95', '811.93', '791.31']\n",
      "['호주', '2019.11.08', '794.68', '810.33', '779.03', '802.62', '786.74', '806.60', '786.10']\n",
      "['호주', '2019.11.07', '798.74', '814.47', '783.01', '806.72', '790.76', '810.72', '790.11']\n",
      "['호주', '2019.11.06', '799.42', '815.16', '783.68', '807.41', '791.43', '811.41', '790.80']\n",
      "['호주', '2019.11.05', '799.89', '815.64', '784.14', '807.88', '791.90', '811.88', '791.27']\n",
      "['호주', '2019.11.04', '801.73', '817.52', '785.94', '809.74', '793.72', '813.75', '793.07']\n",
      "['호주', '2019.11.01', '805.11', '820.97', '789.25', '813.16', '797.06', '817.18', '796.41']\n",
      "['호주', '2019.10.31', '806.03', '821.90', '790.16', '814.09', '797.97', '818.12', '797.31']\n",
      "['호주', '2019.10.30', '801.44', '817.22', '785.66', '809.45', '793.43', '813.46', '792.78']\n",
      "['호주', '2019.10.29', '799.85', '815.60', '784.10', '807.84', '791.86', '811.84', '791.20']\n",
      "['호주', '2019.10.28', '799.22', '814.96', '783.48', '807.21', '791.23', '811.20', '790.58']\n",
      "['호주', '2019.10.25', '801.48', '817.26', '785.70', '809.49', '793.47', '813.50', '792.82']\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "import requests\n",
    "import pandas as pd\n",
    "import csv\n",
    "import datetime\n",
    "\n",
    "URL = 'http://finance.naver.com/marketindex/exchangeDailyQuote.nhn?marketindexCd=FX_'\n",
    "keys = ['USDKRW', 'JPYKRW', 'EURKRW', 'CNYKRW', 'CADKRW', 'AUDKRW']\n",
    "\n",
    "def country():\n",
    "    country_name_list = []\n",
    "    for key in keys:\n",
    "        country_url = 'https://finance.naver.com/marketindex/exchangeDetail.nhn?marketindexCd=FX_'\n",
    "        tempUrl = country_url + key\n",
    "        req = requests.get(tempUrl)\n",
    "        soup = BeautifulSoup(req.text, \"html.parser\")\n",
    "        country_names = soup.select('#container > div.h_company > h2 ')\n",
    "        for country_name in country_names:\n",
    "#             print(country_name.text)\n",
    "            country_name = country_name.text.strip()\n",
    "            country_name_list.append(country_name)\n",
    "    return country_name_list\n",
    "\n",
    "def aud(soup, country_name):\n",
    "#     url = URL + keys[0]\n",
    "#     for i in range(1, 5):\n",
    "#         usd_url = url + '&page=' + str(i)\n",
    "#         req = requests.get(usd_url)\n",
    "#         soup = BeautifulSoup(req.text, \"html.parser\")\n",
    "#         sleep(1)\n",
    "    country()\n",
    "    aud_list = []\n",
    "    tr_list = soup.select('body > div > table > tbody > tr')\n",
    "    for tr in tr_list:\n",
    "        aud_date = tr.find('td', {'class' : 'date'}).text\n",
    "        aud_exchange = tr.find('td', {'class' : 'num'}).text\n",
    "        for aud_buy_cash in tr.select('td:nth-child(4)'):\n",
    "            aud_buy_cash = aud_buy_cash.text\n",
    "        for aud_sell_cash in tr.select('td:nth-child(5)'):\n",
    "            aud_sell_cash = aud_sell_cash.text\n",
    "        for aud_send_money in tr.select('td:nth-child(6)'):\n",
    "            aud_send_money = aud_send_money.text\n",
    "        for aud_get_money in tr.select('td:nth-child(7)'):\n",
    "            aud_get_money = aud_get_money.text\n",
    "        for aud_buy_tc in tr.select('td:nth-child(8)'):\n",
    "            aud_buy_tc = aud_buy_tc.text.strip()\n",
    "        for aud_sell_check in tr.select('td:nth-child(9)'):\n",
    "            aud_sell_check = aud_sell_check.text.strip()\n",
    "        aud_list.append([country_name, aud_date, aud_exchange, aud_buy_cash, aud_sell_cash, aud_send_money, aud_get_money, aud_buy_tc, aud_sell_check])\n",
    "\n",
    "    return aud_list\n",
    "\n",
    "def toCSV(aud_whole_list):\n",
    "    dt = datetime.datetime.now()\n",
    "    file = open('exchange '+dt.strftime('%Y_%m_%d')+'.csv', 'w', encoding='cp949', newline='') #usd.csv로 바꿀것\n",
    "    csvfile = csv.writer(file)\n",
    "    csvfile.writerow(['country', 'date', 'exchange', 'buying cash', 'selling cash', 'sending money', 'getting money', 'buying TC', 'selling check'])\n",
    "    for row in aud_whole_list:\n",
    "        csvfile.writerow(row)\n",
    "    file.close()\n",
    "\n",
    "def toCSV_a(aud_whole_list):\n",
    "    dt = datetime.datetime.now()\n",
    "    with open('test '+dt.strftime('%Y_%m_%d')+'.csv', 'w', encoding='cp949', newline='') as file: #usd.csv로 바꿀것\n",
    "        csvfile = csv.writer(file)\n",
    "        csvfile.writerow(['country', 'date', 'exchange', 'buying cash', 'selling cash', 'sending money', 'getting money', 'buying TC', 'selling check'])\n",
    "        csvfile.writerow(aud_whole_list)\n",
    "    file.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "#     req = requests.get(URL + keys[0]) #0으로 바꿀것\n",
    "#     soup = BeautifulSoup(req.text, \"html.parser\")\n",
    "\n",
    "    country_name = country()\n",
    "\n",
    "    aud_whole_list = []\n",
    "    for i in range(1,3):\n",
    "        aud_url = URL + keys[5] + '&page=' + str(i)\n",
    "        req = requests.get(aud_url)\n",
    "        soup = BeautifulSoup(req.text, \"html.parser\")\n",
    "        sleep(1)\n",
    "        aud_whole_list += aud(soup, country_name[5])\n",
    "#         usd_whole_list += usd(soup, keys[0]) #USDKRW로 출력\n",
    "\n",
    "    for item in aud_whole_list:\n",
    "        print(item)\n",
    "\n",
    "    toCSV(aud_whole_list)\n",
    "\n",
    "#crawling_beautifulsoup에서 usd_get_one_data 함수 가져올 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
